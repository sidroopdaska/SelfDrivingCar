{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a classifer\n",
    "\n",
    "The goal of this Jupyter notebook is to:\n",
    "1. Extract features from the raw data set processed in *rawdata_exploration.ipynb* notebook\n",
    "2. Pre-process the features, and, \n",
    "3. Train a classifier to recognise vehicles in an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "import pandas as pd\n",
    "\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(imgs, params):\n",
    "    '''\n",
    "    Extracts user specified features (either Raw Color values/ Histogram of Color values/ HOG or a combination of these)\n",
    "    from a list of images\n",
    "    :param img ([string]): List of image paths\n",
    "    :param params (dict): Dictionary of different params for feature extraction\n",
    "    :return: List of features vectors\n",
    "    '''\n",
    "    if not params:\n",
    "        raise Exception('ERROR: Please provide a valid params dict!')\n",
    "        \n",
    "    features = []\n",
    "    for img_path in tqdm(imgs):\n",
    "        img = mpimg.imread(img_path)\n",
    "        \n",
    "        features.append(single_img_features(\n",
    "            img,\n",
    "            color_space=params['color_space'],\n",
    "            spatial_size=params['spatial_size'],\n",
    "            hist_bins=params['hist_bins'],\n",
    "            orient=params['orient'],\n",
    "            pix_per_cell=params['pix_per_cell'],\n",
    "            cell_per_block=params['cell_per_block'],\n",
    "            hog_channel=params['hog_channel'],\n",
    "            spatial_feat=params['spatial_feat'],\n",
    "            hist_feat=params['hist_feat'],\n",
    "            hog_feat=params['hog_feat']\n",
    "        ))\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n",
      "/Users/sid/anaconda/envs/carnd-term1/lib/python3.5/site-packages/skimage/feature/_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken to extract features (Spatial, Color Hist, HOG): 112.85023307800293\n"
     ]
    }
   ],
   "source": [
    "# Define the params dictionary\n",
    "params = {\n",
    "    'color_space': 'YCrCb',   # Can be RGB, HSV, LAB, HLS, YUV, YCrCb\n",
    "    'orient': 9,              # HOG orientations\n",
    "    'pix_per_cell': 8,        # HOG pixels per cell\n",
    "    'cell_per_block': 2,      # HOG cells per block\n",
    "    'spatial_size': (16, 16), # Spatial binning dimensions\n",
    "    'hist_bins': 24,          # Number of histogram bins\n",
    "    'hog_channel': 'ALL',     # Can be 0, 1, 2, or \"ALL\"\n",
    "    'spatial_feat': True,     # Spatial features on or off\n",
    "    'hist_feat': True,        # Histogram features on or off\n",
    "    'hog_feat': True,         # HOG features on or off\n",
    "}\n",
    "     \n",
    "# Load pickled raw data set\n",
    "with open('rawdata.p', mode='rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "cars_train = data['cars_train']\n",
    "notcars_train = data['notcars_train']\n",
    "\n",
    "cars_test = data['cars_test']\n",
    "notcars_test = data['notcars_test']\n",
    "\n",
    "# Extract features\n",
    "print('Extracting features...')\n",
    "start = time.time()\n",
    "\n",
    "cars_features_train = extract_features(cars_train, params)\n",
    "notcars_features_train = extract_features(notcars_train, params)\n",
    "\n",
    "cars_features_test = extract_features(cars_test, params)\n",
    "notcars_features_test = extract_features(notcars_test, params)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken to extract features (Spatial, Color Hist, HOG): {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector: 6132\n"
     ]
    }
   ],
   "source": [
    "print('Length of feature vector: {}'.format(len(cars_features_train[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.vstack([cars_features_train, notcars_features_train]).astype(np.float64) \n",
    "X_test = np.vstack([cars_features_test, notcars_features_test]).astype(np.float64) \n",
    "\n",
    "y_train = np.hstack([np.ones(len(cars_features_train)), np.zeros(len(notcars_features_train))])\n",
    "y_test = np.hstack([np.ones(len(cars_features_test)), np.zeros(len(notcars_features_test))])\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Sanity check\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_test, y_test = shuffle(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a classifier (Linear Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training took: 15.9724\n",
      "Training accuracy: 1.0000\n",
      "Test accuracy: 0.9781\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>879</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  879   22\n",
       "1   17  859"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "# clf = LinearSVC(C = 0.00005)\n",
    "# parameters = {'C': sp_randint(0, 20)}\n",
    "# clf = RandomizedSearchCV(lsvc, param_distributions=parameters)\n",
    "\n",
    "print('Starting training...')\n",
    "start = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Training took: {:.4f}'.format(end-start))\n",
    "print('Training accuracy: {:.4f}'.format(clf.score(X_train, y_train)))\n",
    "print('Test accuracy: {:.4f}'.format(clf.score(X_test, y_test)))\n",
    "print()\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "df = pd.DataFrame(confusion_matrix(preds, y_test))\n",
    "print('Confusion Matrix:')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pickled the classifier data!\n"
     ]
    }
   ],
   "source": [
    "# Once happy, pickle the data\n",
    "try:\n",
    "    with open('classifier_data.p', mode='wb') as f:\n",
    "        pickle.dump({\n",
    "            'clf': clf,\n",
    "            'scaler': scaler,\n",
    "            'orient': params['orient'],\n",
    "            'pix_per_cell': params['pix_per_cell'],\n",
    "            'cell_per_block': params['cell_per_block'],\n",
    "            'spatial_size': params['spatial_size'],\n",
    "            'hist_bins': params['hist_bins'],\n",
    "            'color_space': params['color_space']\n",
    "        }, f)\n",
    "        \n",
    "except Exception as e:\n",
    "    print('ERROR: Failed to pickle the classifier and its params with exception: {}'.format(e))\n",
    "    \n",
    "print('Successfully pickled the classifier data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "02d003efb90f4b2388a3c2d2d4de2c75": {
     "views": []
    },
    "12de28f88cb8452f8a15e438c7bbac04": {
     "views": []
    },
    "16a1ca4d1def439a80602ec9242f28a1": {
     "views": []
    },
    "1896d0833b47452a91ad9d3fc27dacfe": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "1e0932733769430daa48030dde37ab59": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "297f035ec76f436c973865366dea059d": {
     "views": []
    },
    "43ab156d66704718a60c5b4b21e1cc75": {
     "views": []
    },
    "477f1b5c353d4448a28fb58743f65e67": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "51e17576e59f40f88fb93c14b966bf8e": {
     "views": []
    },
    "5d83f76605ce48f88b103accd105b409": {
     "views": []
    },
    "62cea0db107d4382b606816388608aae": {
     "views": []
    },
    "64d0d69defb3408aba463d1aa81a0165": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "6ecdefc295b84155a8ec00d6c14e86b1": {
     "views": []
    },
    "7dfa3ebbdf984164b42aa2caedd80d24": {
     "views": []
    },
    "83a744006ad94052bbc86f63478ea7b6": {
     "views": []
    },
    "8a9650d1a08a4eab8a0e7f5114528956": {
     "views": []
    },
    "90f5108a10f848b19452160e9a167330": {
     "views": []
    },
    "94ded227cd9f49f89e5e709a976719c9": {
     "views": []
    },
    "b2bb5c7a2d404f109e9af35818a9cb2b": {
     "views": []
    },
    "ba2a81ac8f2146bbb282b7d8a41601e2": {
     "views": []
    },
    "d743718881da496d913d7a4ada191c3b": {
     "views": []
    },
    "dde79f1fdfc74e349a36f5117779cdf4": {
     "views": []
    },
    "e67b4f57e18a4f5095dbf0808a56c183": {
     "views": []
    },
    "e7913c1afce94b58a004b63fc742c6ef": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
